import pandas as pd
import time
import random
from google_play_scraper import search, app, Sort
from tqdm import tqdm

class PlayStoreScraper:
    def __init__(self, lang='ja', country='jp'):
        self.lang = lang
        self.country = country
        self.apps_data = []

    def search_app_ids(self, queries, limit=50):
        """
        ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã§ã‚¢ãƒ—ãƒªIDï¼ˆãƒ‘ãƒƒã‚±ãƒ¼ã‚¸åï¼‰ã®ãƒªã‚¹ãƒˆã‚’åé›†ã™ã‚‹
        """
        app_ids = set() # é‡è¤‡ã‚’é˜²ããŸã‚ã«setã‚’ä½¿ç”¨
        print(f"ğŸ” æ¤œç´¢ã‚’é–‹å§‹ã—ã¾ã™: {queries}")

        for query in queries:
            try:
                results = search(
                    query,
                    lang=self.lang,
                    country=self.country,
                    n_hits=limit # å„ã‚¯ã‚¨ãƒªã§ä½•ä»¶å–å¾—ã™ã‚‹ã‹
                )
                for res in results:
                    app_ids.add(res['appId'])
                
                print(f"  - '{query}' ã§ {len(results)} ä»¶å–å¾—")
                time.sleep(random.uniform(1, 3)) # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›ã®ãŸã‚ã®å¾…æ©Ÿ
            except Exception as e:
                print(f"Error searching {query}: {e}")
        
        print(f"âœ… åˆè¨ˆ {len(app_ids)} å€‹ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªã‚¢ãƒ—ãƒªIDã‚’åé›†ã—ã¾ã—ãŸã€‚")
        return list(app_ids)

    def fetch_details(self, app_ids):
        """
        ã‚¢ãƒ—ãƒªIDã®ãƒªã‚¹ãƒˆã‹ã‚‰è©³ç´°æƒ…å ±ï¼ˆèª¬æ˜æ–‡ã€æ¨©é™ãªã©ï¼‰ã‚’å–å¾—ã™ã‚‹
        """
        print("ğŸ“¥ è©³ç´°æƒ…å ±ã®å–å¾—ã‚’é–‹å§‹ã—ã¾ã™...")
        
        for app_id in tqdm(app_ids): # é€²æ—ãƒãƒ¼ã‚’è¡¨ç¤º
            try:
                # APIãƒªã‚¯ã‚¨ã‚¹ãƒˆ
                details = app(
                    app_id,
                    lang=self.lang,
                    country=self.country
                )
                
                # å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ã ã‘ã‚’æŠ½å‡ºã—ã¦è¾æ›¸ã«ã™ã‚‹
                extracted_data = {
                    'app_id': details.get('appId'),
                    'title': details.get('title'),
                    'description': details.get('description'), # BERTç”¨é‡è¦ãƒ‡ãƒ¼ã‚¿
                    'summary': details.get('summary'),
                    'score': details.get('score'),
                    'ratings': details.get('ratings'),
                    'reviews': details.get('reviews'), # ãƒ¬ãƒ“ãƒ¥ãƒ¼æ•°
                    'installs': details.get('installs'),
                    'developer': details.get('developer'),
                    'developer_email': details.get('developerEmail'),
                    'developer_website': details.get('developerWebsite'),
                    'updated': details.get('updated'),
                    'contains_ads': details.get('adSupported'), # åºƒå‘Šã®æœ‰ç„¡
                    # æ¨©é™ãƒªã‚¹ãƒˆã¯AIå­¦ç¿’ç”¨ã«æ–‡å­—åˆ—åŒ–ã—ã¦ä¿å­˜
                    'permissions': ",".join([p['permission'] for p in details.get('permissions') or []]),
                    'icon_url': details.get('icon')
                }
                
                self.apps_data.append(extracted_data)
                
                # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ¤œçŸ¥å›é¿ã®ãŸã‚ãƒ©ãƒ³ãƒ€ãƒ ã«å¾…æ©Ÿ
                time.sleep(random.uniform(0.5, 1.5))

            except Exception as e:
                # å‰Šé™¤ã•ã‚ŒãŸã‚¢ãƒ—ãƒªãªã©ã§ã‚¨ãƒ©ãƒ¼ãŒå‡ºã¦ã‚‚æ­¢ã¾ã‚‰ãªã„ã‚ˆã†ã«ã™ã‚‹
                # print(f"Error fetching {app_id}: {e}")
                continue

    def save_to_csv(self, filename='dataset.csv'):
        """ãƒ‡ãƒ¼ã‚¿ã‚’CSVã«ä¿å­˜"""
        if not self.apps_data:
            print("ä¿å­˜ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            return
            
        df = pd.DataFrame(self.apps_data)
        # ãƒ©ãƒ™ãƒ«åˆ—ï¼ˆis_fraudï¼‰ã¯å¾Œã§äººé–“ã¾ãŸã¯ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã§åŸ‹ã‚ã‚‹ãŸã‚ç©ºã‘ã¦ãŠãã€ã¾ãŸã¯ä»®ç½®ã
        if 'is_fraud' not in df.columns:
            df['is_fraud'] = -1 # -1: æœªåˆ¤å®š
            
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"ğŸ’¾ ãƒ‡ãƒ¼ã‚¿ã‚’ {filename} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚ ({len(df)}ä»¶)")

# --- å®Ÿè¡Œéƒ¨ ---
if __name__ == "__main__":
    scraper = PlayStoreScraper()
    
    # 1. è©æ¬ºã‚¢ãƒ—ãƒªãŒå¤šãã†ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼‰
    risky_keywords = [
        "phone cleaner", "battery booster", "cpu cooler", 
        "ram booster", "virus cleaner", "free antivirus",
        "ã‚¹ãƒãƒ›æœ€é©åŒ–", "ãƒãƒƒãƒ†ãƒªãƒ¼é•·æŒã¡"
    ]
    
    # 2. æ­£å¸¸ãªã‚¢ãƒ—ãƒªã‚‚æ··ãœã‚‹ãŸã‚ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆæ¯”è¼ƒå¯¾è±¡ï¼‰
    safe_keywords = [
        "browser", "camera", "clock", "calculator", "SNS", "news"
    ]
    
    # IDåé›†
    target_ids = scraper.search_app_ids(risky_keywords, limit=30)
    safe_ids = scraper.search_app_ids(safe_keywords, limit=10) # ãƒãƒ©ãƒ³ã‚¹èª¿æ•´
    
    all_ids = target_ids + safe_ids
    
    # è©³ç´°å–å¾—
    scraper.fetch_details(all_ids)
    
    # CSVä¿å­˜
    scraper.save_to_csv("app_dataset_raw.csv")