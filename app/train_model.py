import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import BertJapaneseTokenizer, BertModel
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score
import gc  # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ç”¨
from tqdm import tqdm # é€²æ—ãƒãƒ¼

# --- è¨­å®š ---
BATCH_SIZE = 8       # GPUãƒ¡ãƒ¢ãƒªä¸è¶³ã§ã‚¨ãƒ©ãƒ¼ãŒå‡ºã‚‹å ´åˆã¯ã“ã“ã‚’ 8 ã‚„ 4 ã«ä¸‹ã’ã‚‹
EPOCHS = 3            # å­¦ç¿’å›æ•°
LEARNING_RATE = 2e-5  # å­¦ç¿’ç‡
MAX_LEN = 128         # æ–‡ç« ã®æœ€å¤§é•·ï¼ˆé•·ãã™ã‚‹ã¨ãƒ¡ãƒ¢ãƒªã‚’é£Ÿã†ï¼‰
MODEL_NAME = 'cl-tohoku/bert-base-japanese-v3'

# --- ãƒ‡ãƒã‚¤ã‚¹ã®è‡ªå‹•é¸æŠ ---
def get_device():
    if torch.cuda.is_available():
        device_name = torch.cuda.get_device_name(0)
        print(f"ğŸš€ GPUãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ: {device_name}")
        return torch.device("cuda")
    else:
        print("âš ï¸ GPUãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚CPUã§å­¦ç¿’ã—ã¾ã™ï¼ˆæ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼‰ã€‚")
        return torch.device("cpu")

DEVICE = get_device()

# --- 1. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå®šç¾© ---
class AppDataset(Dataset):
    def __init__(self, df, tokenizer, max_len):
        self.df = df
        self.tokenizer = tokenizer
        self.max_len = max_len
        
        # æ¨©é™ãƒªã‚¹ãƒˆã®ç‰¹å®šï¼ˆç°¡æ˜“ç‰ˆï¼šå®Ÿéš›ã¯ã‚‚ã£ã¨å¤šãã®æ¨©é™ã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã™ã‚‹ï¼‰
        self.risky_perms = [
            "SYSTEM_ALERT_WINDOW", "RECEIVE_BOOT_COMPLETED", 
            "BIND_ACCESSIBILITY_SERVICE", "READ_CONTACTS"
        ]

    def __len__(self):
        return len(self.df)

    def __getitem__(self, index):
        row = self.df.iloc[index]
        
        # ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†
        text = str(row['description']) if pd.notna(row['description']) else ""
        
        # æ¨©é™ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†ï¼ˆOne-hotã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°çš„ãªã‚‚ã®ï¼‰
        perms_str = str(row['permissions']) if pd.notna(row['permissions']) else ""
        perm_vec = [1.0 if rp in perms_str else 0.0 for rp in self.risky_perms]
        
        # BERTç”¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
        encoding = self.tokenizer(
            text,
            add_special_tokens=True,
            max_length=self.max_len,
            return_token_type_ids=False,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt',
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'metadata': torch.tensor(perm_vec, dtype=torch.float),
            'labels': torch.tensor(row['is_fraud'], dtype=torch.float)
        }

# --- 2. ãƒ¢ãƒ‡ãƒ«å®šç¾© (ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å‹) ---
class FraudDetector(nn.Module):
    def __init__(self, n_meta_features):
        super(FraudDetector, self).__init__()
        self.bert = BertModel.from_pretrained(MODEL_NAME)
        
        # BERTã®å‡ºåŠ›å±¤ç›´å¾Œã«ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã‚’å…¥ã‚Œã‚‹ï¼ˆéå­¦ç¿’é˜²æ­¢ï¼‰
        self.drop = nn.Dropout(p=0.3)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å‡¦ç†ç”¨ã®å±¤
        self.meta_layer = nn.Linear(n_meta_features, 32)
        
        # æœ€çµ‚åˆ†é¡å±¤ (BERTã®768æ¬¡å…ƒ + ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®32æ¬¡å…ƒ)
        self.out = nn.Linear(768 + 32, 1)

    def forward(self, input_ids, attention_mask, metadata):
        # BERTã®å‡¦ç†
        outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask
        )
        # pooler_outputã¯[CLS]ãƒˆãƒ¼ã‚¯ãƒ³ã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«
        bert_out = self.drop(outputs.pooler_output)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†
        meta_out = torch.relu(self.meta_layer(metadata))
        
        # çµåˆ
        combined = torch.cat((bert_out, meta_out), dim=1)
        
        # æœ€çµ‚å‡ºåŠ›
        return self.out(combined)

# --- 3. å­¦ç¿’ãƒ«ãƒ¼ãƒ—é–¢æ•° ---
def train_epoch(model, data_loader, loss_fn, optimizer, scheduler, n_examples):
    model = model.train()
    losses = []
    correct_predictions = 0
    
    # æ··åˆç²¾åº¦å­¦ç¿’ã®ãŸã‚ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼
    scaler = torch.amp.GradScaler('cuda',enabled=torch.cuda.is_available())

    for d in tqdm(data_loader, desc="Training"):
        input_ids = d["input_ids"].to(DEVICE)
        attention_mask = d["attention_mask"].to(DEVICE)
        metadata = d["metadata"].to(DEVICE)
        labels = d["labels"].to(DEVICE)

        optimizer.zero_grad()

        # æ··åˆç²¾åº¦ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆãƒ¡ãƒ¢ãƒªç¯€ç´„ï¼†é«˜é€ŸåŒ–ï¼‰
        with torch.amp.autocast('cuda',enabled=torch.cuda.is_available()):
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                metadata=metadata
            )
            # sigmoidã§0~1ã«ã—ã¦ã‹ã‚‰æå¤±è¨ˆç®—ã—ãŸã„ãŒã€
            # BCEWithLogitsLossã‚’ä½¿ã†ã®ã§ç”Ÿã®å‡ºåŠ›(logit)ã‚’æ¸¡ã™ã®ãŒå®‰å®šçš„
            loss = loss_fn(outputs, labels.unsqueeze(1))

        # èª¤å·®é€†ä¼æ’­
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
        
        if scheduler:
            scheduler.step()

        preds = torch.sigmoid(outputs).round()
        correct_predictions += torch.sum(preds.flatten() == labels)
        losses.append(loss.item())
        
        # GPUãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’é©å®œã‚¯ãƒªã‚¢ï¼ˆãŠã¾ã˜ãªã„ï¼‰
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    return correct_predictions.double() / n_examples, np.mean(losses)

# --- ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œéƒ¨ ---
def main():
    try:
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        print("ğŸ“‚ ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™...")
        df = pd.read_csv(r'C:\learn\scamwall\app_dataset_labeled.csv')
        
        # ãƒ‡ãƒ¼ã‚¿æ•°ãŒå°‘ãªã™ãã‚‹ã¨ã‚¨ãƒ©ãƒ¼ã«ãªã‚‹ã®ã§ãƒã‚§ãƒƒã‚¯
        if len(df) < 10:
            print("âš ï¸ ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã™ãã¾ã™ã€‚data_collector.pyã§ã‚‚ã£ã¨é›†ã‚ã¦ãã ã•ã„ã€‚")
            return

        # è¨“ç·´ç”¨ã¨ãƒ†ã‚¹ãƒˆç”¨ã«åˆ†å‰² (8:2)
        df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)
        
        tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)
        
        # DataLoaderä½œæˆ
        train_dataset = AppDataset(df_train, tokenizer, MAX_LEN)
        test_dataset = AppDataset(df_test, tokenizer, MAX_LEN)
        
        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE) # ãƒ†ã‚¹ãƒˆæ™‚ã¯ã‚·ãƒ£ãƒƒãƒ•ãƒ«ä¸è¦

        # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
        # æ¨©é™ç‰¹å¾´é‡ã®æ•°ã¯ AppDataset.risky_perms ã®é•·ã•ã¨åŒã˜ã«ã™ã‚‹
        model = FraudDetector(n_meta_features=4) 
        model = model.to(DEVICE)

        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶è¨­å®š
        optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)
        loss_fn = nn.BCEWithLogitsLoss().to(DEVICE) # 2å€¤åˆ†é¡ç”¨ãƒ­ã‚¹é–¢æ•°

        print("ğŸ”¥ å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™...")
        
        for epoch in range(EPOCHS):
            print(f'Epoch {epoch + 1}/{EPOCHS}')
            print('-' * 10)

            try:
                train_acc, train_loss = train_epoch(
                    model,
                    train_loader,
                    loss_fn,
                    optimizer,
                    None, # Schedulerã¯ä»Šå›ã¯çœç•¥
                    len(df_train)
                )
                print(f'Train loss {train_loss} accuracy {train_acc}')
                
            except RuntimeError as e:
                if 'out of memory' in str(e):
                    print("âŒ GPUãƒ¡ãƒ¢ãƒªä¸è¶³ãŒç™ºç”Ÿã—ã¾ã—ãŸï¼")
                    print("å¯¾ç­–: BATCH_SIZE ã‚’å°ã•ãã—ã¦ãã ã•ã„ï¼ˆç¾åœ¨: {}ï¼‰".format(BATCH_SIZE))
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    break
                else:
                    raise e

        print("âœ… å­¦ç¿’å®Œäº†ã€‚ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã™ã€‚")
        torch.save(model.state_dict(), 'fraud_model.pth')
        print("ğŸ’¾ fraud_model.pth ã¨ã—ã¦ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚")

    except FileNotFoundError:
        print("ã‚¨ãƒ©ãƒ¼: app_dataset_labeled.csv ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚labeler.py ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

if __name__ == "__main__":
    main()